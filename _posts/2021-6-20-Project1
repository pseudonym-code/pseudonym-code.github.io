---
layout: post
title: ST558 Project 1 Reflection
---
For this project, I created user functions that are able to access endpoints on both the [NHL Records API](https://gitlab.com/dword4/nhlapi/-/blob/master/records-api.md) and the [NHL Stats API](https://gitlab.com/dword4/nhlapi/-/blob/master/stats-api.md). These user functions are able to query and format the data from these APIs into simple tibbles in R. This was done using many packages available such as `jsonlite` and `RSQLite` to parse the data and convert it. I then used portions of the data from the APIs to perform exploratory data analysis. The data I used was complete historic franchise data for the 31 active NHL teams, and merged this data with the 31 team's current season statistics. I was then able to compare the historic rates to current ones. This was really interesting as there were many intriguing findings around certain teams. One in particular, the Arizona Coyotes (ARI), have a very low total goal count compared to others in the same timeline. This may be a data issue but would be something to look into.  

It was also interesting to look into the differences between divisions. Historically, there is a large difference between the division's goals per game spread, but this current season they were almost identical across the 4 divisions. This was unexpected. Another interesting note is that some of the highest historic win percentages come from the same division as teams with the lowest historic win percentages (West division). As teams within divisions play each other the most throughout the seasons, it would be interesting to know whether the good teams are getting higher win rates due to being paired with the worst teams in the league, or if the worst teams are at the bottom because they have to play the best teams more often. Most likely it comes down to a mix of the two, but it was intriguing none-the-less. 

The entire project was tracked and stored on GitHub, connected through RStudio. This allowed for easy access to the file and an easy way to stay on top of the process. Being able to document each step and push it to the cloud knowing that your file is safe is a big positive. I am very excited to have learned this as when I finish my masterâ€™s degree in August, I hope to continue to post things like the vignette the project produced. Understanding the capabilities behind R and GitHub has been a real positive from this project. I am hoping to continue to learn and post tutorials and lessons on my GitHub account weekly.

The hardest part of this project was getting the GitHub connection set up and then connecting to the APIs. I have used set APIs like Google Maps and others before, but I have not had to do it quite like this before with the URL manipulation. This was hard to be able to figure out the best way to obtain the data I wanted. In the end, I was able to pull the data and put together some interesting analysis. 

Now that I have this experience, I believe that I will be able to better connect to APIs in the future. Writing the individual API functions and then a wrapper function over it all was a good experience to take into future projects like this one, especially when working with people that may not be as technologically savvy. Allowing just one entry point for an API rather than multiple is far easier for everyone. The data analysis portion was interesting as well and gave me an opportunity to truly think through exactly what I wanted to explore and how the data could answer my questions. All in all, this was a great experience that I will be able to take with me and use in the future. 

To view this project in full, visit the Github Pages site [here](https://pseudonym-code.github.io/ST558-Project1/) or the Project [repository](https://github.com/pseudonym-code/ST558-Project1).

Thanks!
